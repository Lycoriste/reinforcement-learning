{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo as Monitor, ResizeObservation, GrayscaleObservation\n",
    "import ale_py\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple, defaultdict\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "env = ResizeObservation(env, (84, 84))\n",
    "env = GrayscaleObservation(env, keep_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        # Channel, height, and width\n",
    "        channel, _, _ = state_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Dynamically calculate output size for fc1\n",
    "\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize x -> (0, 255)\n",
    "        x /= 255.0\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, batch_size = 128, gamma = 0.99, epsilon_start = 0.99, epsilon_end = 0.9, epsilon_decay = 1500, tau = 0.005, alpha = 1e-4):\n",
    "        # Hyperparameters\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Utilities\n",
    "        self.memory = ReplayMemory(batch_size * 100)\n",
    "        self.steps = 0\n",
    "        self.episode_reward = defaultdict(int)\n",
    "        self.episode_length = defaultdict(int)\n",
    "        self.episodes_total = 0\n",
    "\n",
    "        # This is a continuous observation space (env.observation_space.n for Discrete)\n",
    "        # One neural network for optimal policy and the other for behavior/exploration\n",
    "        state_space = env.observation_space.shape\n",
    "        state_space = (state_space[2], state_space[0], state_space[1])\n",
    "        self.policy_net = DQN(state_space, env.action_space.n).to(device)\n",
    "        self.target_net = DQN(state_space, env.action_space.n).to(device)\n",
    "        # Apply policy_net parameters (weights and biases) to target_net\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        # Adam - SGD either one works\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.alpha)\n",
    "\n",
    "    # Epsilon greedy action selection\n",
    "    def select_action(self, state):\n",
    "        # Epsilon Threshold: Early in training -> prioritize exploring | Later in training -> exploit more\n",
    "        eps_threshold = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                        math.exp(-1.0 * self.steps / self.epsilon_decay)\n",
    "        self.steps += 1\n",
    "\n",
    "        # If random number (0, 1) less than eps_threshold then we explore, else we exploit\n",
    "        if random.random() < eps_threshold:\n",
    "            # Explore: we choose a random action\n",
    "            return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "        else:\n",
    "            # Exploit: we choose the best action at the state\n",
    "            with torch.inference_mode(): \n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "            \n",
    "    # Optimize agent's neural network\n",
    "    def optimize_model(self):\n",
    "        # Can't optimize model when replay memory is not filled\n",
    "        if (len(self.memory) < self.batch_size):\n",
    "            return\n",
    "        # Sample transitions from replay memory\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose batch: convert batch-array of transitions to transition of batch-arrays\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Filter out all the next_state that are the endpoints of an episode\n",
    "        next_state_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), \n",
    "                                        device=device, \n",
    "                                        dtype=torch.bool)\n",
    "        \n",
    "        # Concatenates each of the transitions (state, action, reward) into their respective batches\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        next_state_batch = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        # Get actions from states along the action vector\n",
    "        action_value = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Q(s', a') for all next_states that are not None\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.inference_mode():\n",
    "            # Double DQN -> Q_t(s', argmax Q_p(s', a'))\n",
    "            next_state_values[next_state_mask] = self.target_net(next_state_batch).max(1).values #gather(1, self.policy_net(next_state_batch).argmax(1, keepdim=True)).squeeze(1)\n",
    "        \n",
    "        # If the next_state is None, then the Q(s', a') term evaluates to 0\n",
    "        expected_action_value = next_state_values * self.gamma + reward_batch\n",
    "\n",
    "        # Loss function for DQN -> L(s) = MSE\n",
    "        # We use the Huber loss here as it is less sensitive to outliers\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "        loss = loss_fn(action_value, expected_action_value.unsqueeze(1))\n",
    "\n",
    "\n",
    "        # Clear all gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping - prevents them from exploding\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft updates\n",
    "        target_net_state_dict = self.target_net.state_dict()\n",
    "        policy_net_state_dict = self.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1 - self.tau)\n",
    "        self.target_net.load_state_dict(target_net_state_dict)\n",
    "    \n",
    "    # Train agent - for any actual improvements we need more than 128 episodes as one batch = 128\n",
    "    def train(self, episodes: int = 128):\n",
    "        plt.ion()\n",
    "        fig, ax = plt.subplots()\n",
    "        for ep in range(episodes):\n",
    "            state_raw, _ = env.reset()\n",
    "\n",
    "            while True:\n",
    "                state_tensor = torch.tensor(state_raw, dtype=torch.float32, device=device)\n",
    "                state_tensor = state_tensor.permute(2, 0 ,1).unsqueeze(0)\n",
    "                action_tensor = self.select_action(state_tensor)\n",
    "                action = action_tensor.item()\n",
    "\n",
    "                next_state_raw, reward, done, truncated, _ = env.step(action)\n",
    "                next_state_tensor = torch.tensor(next_state_raw, dtype=torch.float32, device=device) \n",
    "                next_state_tensor = next_state_tensor.permute(2, 0 ,1).unsqueeze(0)\n",
    "                reward_tensor = torch.tensor([reward], device=device)\n",
    "\n",
    "                self.memory.push(state_tensor, action_tensor, next_state_tensor, reward_tensor)\n",
    "\n",
    "                self.episode_reward[self.episodes_total] += reward\n",
    "                self.episode_length[self.episodes_total] += 1\n",
    "                self.steps += 1\n",
    "\n",
    "                self.optimize_model()\n",
    "\n",
    "                if done or truncated:\n",
    "                    break\n",
    "                \n",
    "                state_raw = next_state_raw\n",
    "\n",
    "            # Update the plot every 10 episodes once training starts\n",
    "            if (ep % 5 == 0):\n",
    "                ax.clear()  # Clears the old plot\n",
    "                ax.plot(list(self.episode_length.keys()), list(self.episode_length.values()), 'b-')  # Plots the updated data\n",
    "                ax.set_xlabel(\"Episodes\")\n",
    "                ax.set_ylabel(\"Episode Length\")\n",
    "                ax.set_title(\"Episode Length Over Time\")\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(fig)\n",
    "\n",
    "            self.episodes_total += 1\n",
    "\n",
    "        return self.episodes_total, self.episode_length, self.episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_0 = Agent()\n",
    "agent_0.train(episodes=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
